# 8puzzle-solver
## 1. M·ª•c ti√™u
X√¢y d·ª±ng m·ªôt c√¥ng c·ª• tr·ª±c quan gi√∫p ng∆∞·ªùi d√πng gi·∫£i b√†i to√°n 8-Puzzle b·∫±ng nhi·ªÅu thu·∫≠t to√°n t√¨m ki·∫øm kh√°c nhau, trong c·∫£ m√¥i tr∆∞·ªùng c√≥ th·ªÉ quan s√°t v√† m√¥i tr∆∞·ªùng kh√¥ng c√≥ quan s√°t (s·ª≠ d·ª•ng tr·∫°ng th√°i ni·ªÅm tin ‚Äì Belief States).

## 2. N·ªôi dung

### 2.1. C√°c thu·∫≠t to√°n T√¨m ki·∫øm kh√¥ng c√≥ th√¥ng tin (BFS, DFS, IDS, UCS)

#### 2.1.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm
Trong b√†i to√°n 8-Puzzle, khi √°p d·ª•ng c√°c thu·∫≠t to√°n t√¨m ki·∫øm kh√¥ng c√≥ th√¥ng tin, c√°c th√†nh ph·∫ßn ch√≠nh ƒë∆∞·ª£c x√°c ƒë·ªãnh nh∆∞ sau:

Tr·∫°ng th√°i (State) l√† m·ªôt b·∫£ng 3x3 ƒë·∫°i di·ªán cho v·ªã tr√≠ c√°c √¥ s·ªë t·ª´ 1‚Äì8 v√† m·ªôt √¥ tr·ªëng (0).

Tr·∫°ng th√°i ban ƒë·∫ßu (Initial state):	Tr·∫°ng th√°i ƒë·∫ßu v√†o m√† ng∆∞·ªùi ch∆°i cung c·∫•p.

Tr·∫°ng th√°i ƒë√≠ch (Goal state): Tr·∫°ng th√°i mong mu·ªën, th∆∞·ªùng l√†: [[1,2,3],[4,5,6],[7,8,0]]

T·∫≠p h√†nh ƒë·ªông (Actions): C√°c thao t√°c di chuy·ªÉn √¥ tr·ªëng: l√™n (‚Üë), xu·ªëng (‚Üì), tr√°i (‚Üê), ph·∫£i (‚Üí)

H√†m k·∫ø ti·∫øp (Successor function):	Cho bi·∫øt tr·∫°ng th√°i m·ªõi sau khi th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông h·ª£p l·ªá.

Ki·ªÉm tra m·ª•c ti√™u (Goal test):	Ki·ªÉm tra xem tr·∫°ng th√°i hi·ªán t·∫°i c√≥ kh·ªõp v·ªõi tr·∫°ng th√°i ƒë√≠ch kh√¥ng.

Chi ph√≠ b∆∞·ªõc ƒëi (Path cost):	M·ªói b∆∞·ªõc ƒëi c√≥ th·ªÉ c√≥ chi ph√≠ b·∫±ng 1 (Uniform) ho·∫∑c t√≠nh ri√™ng.

#### 2.1.2. Gi·∫£i ph√°p (Solution) l√† g√¨?
Gi·∫£i ph√°p l√† m·ªôt chu·ªói h√†nh ƒë·ªông (move sequence) t·ª´ tr·∫°ng th√°i ban ƒë·∫ßu ƒë·∫øn tr·∫°ng th√°i ƒë√≠ch.

V√≠ d·ª•: [‚Üì, ‚Üí, ‚Üì, ‚Üê, ...]

Trong giao di·ªán ch∆∞∆°ng tr√¨nh, m·ªói h√†nh ƒë·ªông ƒë∆∞·ª£c √°p d·ª•ng v√† hi·ªÉn th·ªã k√®m tr·∫°ng th√°i t∆∞∆°ng ·ª©ng.

M·ªôt gi·∫£i ph√°p t·ªët n√™n c√≥ √≠t b∆∞·ªõc v√† chi ph√≠ nh·ªè nh·∫•t (n·∫øu x√©t chi ph√≠).

#### 2.1.3. Nh·∫≠n x√©t v·ªÅ hi·ªáu su·∫•t c√°c thu·∫≠t to√°n trong nh√≥m n√†y

![Uninformed Search](images/uninformed_search_results.jpg)

| Thu·∫≠t to√°n | ∆Øu ƒëi·ªÉm                                                    | Nh∆∞·ª£c ƒëi·ªÉm                               | Quan s√°t th·ª±c t·∫ø                         |
| ---------- | ---------------------------------------------------------- | ---------------------------------------- | ---------------------------------------- |
| **BFS**    | Lu√¥n t√¨m l·ªùi gi·∫£i ng·∫Øn nh·∫•t (n·∫øu chi ph√≠ b·∫±ng nhau)        | T·ªën b·ªô nh·ªõ nhi·ªÅu                         | M·∫•t \~0.73s, 23 b∆∞·ªõc                     |
| **DFS**    | B·ªô nh·ªõ th·∫•p                                                | D·ªÖ ƒëi l·∫°c, kh√¥ng ƒë·∫£m b·∫£o l·ªùi gi·∫£i t·ªëi ∆∞u | Ch·∫≠m h∆°n, m·∫•t \~5.35s                    |
| **UCS**    | T√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i t·ªëi ∆∞u n·∫øu chi ph√≠ kh√¥ng b·∫±ng nhau       | C·∫ßn h√†ng ƒë·ª£i ∆∞u ti√™n, b·ªô nh·ªõ cao         | \~1.27s, gi·∫£i ƒë√∫ng                       |
| **IDS**    | K·∫øt h·ª£p ∆∞u ƒëi·ªÉm c·ªßa DFS v√† BFS (√≠t b·ªô nh·ªõ, ƒë·∫£m b·∫£o t·ªëi ∆∞u) | T·ªën th·ªùi gian do l·∫∑p l·∫°i nhi·ªÅu l·∫ßn       | Nhanh (\~0.39s), nh∆∞ng d√†i h∆°n (27 b∆∞·ªõc) |

‚úÖ Nh·∫≠n x√©t t·ªïng quan:
V·ªõi 8-Puzzle, do kh√¥ng gian tr·∫°ng th√°i kh√¥ng qu√° l·ªõn, BFS ho·∫∑c IDS th∆∞·ªùng l√† l·ª±a ch·ªçn t·ªët.

DFS √≠t hi·ªáu qu·∫£ v√¨ d·ªÖ ƒëi sai h∆∞·ªõng v√† t·ªën th·ªùi gian.

UCS h·ªØu √≠ch n·∫øu b√†i to√°n c√≥ chi ph√≠ b∆∞·ªõc ƒëi kh√°c nhau.

IDS l√† l·ª±a ch·ªçn h·ª£p l√Ω n·∫øu mu·ªën gi·∫£m b·ªô nh·ªõ m√† v·∫´n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i ng·∫Øn.

### 2.2. C√°c thu·∫≠t to√°n T√¨m ki·∫øm c√≥ th√¥ng tin (A* Search, IDA*, Greedy Best-First Search)

![Informed Search](images/informed_search_results.jpg)

| Thu·∫≠t to√°n                   | ∆Øu ƒëi·ªÉm n·ªïi b·∫≠t                                               | Nh∆∞·ª£c ƒëi·ªÉm                                              | Quan s√°t th·ª±c t·∫ø                                      |
| ---------------------------- | ------------------------------------------------------------- | ------------------------------------------------------- | ----------------------------------------------------- |
| **Greedy Best-First Search** | Nhanh, th∆∞·ªùng t√¨m l·ªùi gi·∫£i s·ªõm nh·ªù heuristic (h√†m ƒë√°nh gi√°)   | Kh√¥ng ƒë·∫£m b·∫£o t√¨m l·ªùi gi·∫£i t·ªëi ∆∞u (ch·ªçn ng·∫Øn nh∆∞ng sai) | Th·ªùi gian th·∫•p (\~0.23s), nh∆∞ng d√†i t·ªõi 79 b∆∞·ªõc       |
| **A\***                      | T√¨m l·ªùi gi·∫£i ng·∫Øn nh·∫•t n·∫øu heuristic t·ªët (t·ªëi ∆∞u, ho√†n ch·ªânh) | C·∫ßn nhi·ªÅu b·ªô nh·ªõ ƒë·ªÉ l∆∞u t·∫•t c·∫£ tr·∫°ng th√°i duy·ªát qua     | C√¢n b·∫±ng gi·ªØa t·ªëc ƒë·ªô v√† ch·∫•t l∆∞·ª£ng (\~0.47s, 23 b∆∞·ªõc) |
| **IDA\***                    | K·∫øt h·ª£p ƒë·ªô s√¢u v√† heuristic ‚Üí gi·∫£m b·ªô nh·ªõ so v·ªõi A\*          | C√≥ th·ªÉ ch·∫°y l·∫°i nhi·ªÅu l·∫ßn ‚Üí t·ªën th·ªùi gian h∆°n A\*       | K·∫øt qu·∫£ ƒë√∫ng (24 b∆∞·ªõc), th·ªùi gian \~0.70s             |

‚úÖ T·ªïng k·∫øt:

Greedy tuy nhanh nh·∫•t nh∆∞ng t·∫°o ra l·ªùi gi·∫£i k√©m ch·∫•t l∆∞·ª£ng nh·∫•t (nhi·ªÅu b∆∞·ªõc) do ch·ªâ quan t√¢m ƒë·∫øn heuristic m√† b·ªè qua chi ph√≠ th·ª±c.

A* l√† l·ª±a ch·ªçn t·ªëi ∆∞u nh·∫•t trong tr∆∞·ªùng h·ª£p n√†y: v·ª´a nhanh, v·ª´a t√¨m l·ªùi gi·∫£i ng·∫Øn.

IDA* l√† m·ªôt thay th·∫ø cho A* khi b·ªô nh·ªõ h·∫°n ch·∫ø, ch·∫•p nh·∫≠n th·ªùi gian l√¢u h∆°n m·ªôt ch√∫t.

üß† Ghi ch√∫:

T·∫•t c·∫£ c√°c thu·∫≠t to√°n n√†y d√πng heuristic, ph·ªï bi·∫øn nh·∫•t l√†:

h(n) = t·ªïng kho·∫£ng c√°ch Manhattan t·ª´ m·ªói √¥ v·ªÅ ƒë√∫ng v·ªã tr√≠ trong goal.

A*, IDA* d√πng f(n) = g(n) + h(n)

Greedy ch·ªâ d√πng f(n) = h(n)

### 2.3. C√°c thu·∫≠t to√°n T√¨m ki·∫øm c·ª•c b·ªô
(Simple Hill Climbing, Steepest Ascent Hill Climbing, Stochastic Hill Climbing, Simulated Annealing, Local Beam Search, Genetic Algorithm)

![Local Search Results](images/local_search_results.jpg)

üìå Nh·∫≠n x√©t v·ªÅ hi·ªáu su·∫•t khi √°p d·ª•ng v√†o tr√≤ ch∆°i 8 √¥ ch·ªØ:

| Thu·∫≠t to√°n                        | ∆Øu ƒëi·ªÉm                                             | Nh∆∞·ª£c ƒëi·ªÉm                                                         | Quan s√°t th·ª±c t·∫ø                              |
| --------------------------------- | --------------------------------------------------- | ------------------------------------------------------------------ | --------------------------------------------- |
| **Simple Hill Climbing**          | C√°ch c√†i ƒë·∫∑t ƒë∆°n gi·∫£n, ch·∫°y nhanh                   | D·ªÖ k·∫πt ·ªü local maxima, kh√¥ng ƒë·∫£m b·∫£o t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i             | Nhanh (\~0.05s), gi·∫£i kh√¥ng t·ªëi ∆∞u (114 b∆∞·ªõc) |
| **Steepest Ascent Hill Climbing** | C·∫£i ti·∫øn h∆°n: ch·ªçn n∆∞·ªõc ƒëi t·ªët nh·∫•t trong h√†ng x√≥m  | V·∫´n c√≥ th·ªÉ k·∫πt n·∫øu t·∫•t c·∫£ h√†ng x√≥m k√©m h∆°n                         | Th∆∞·ªùng gi·∫£i ƒë∆∞·ª£c, b∆∞·ªõc dao ƒë·ªông (50‚Äì96 b∆∞·ªõc)  |
| **Stochastic Hill Climbing**      | Ch·ªçn ng·∫´u nhi√™n h√†ng x√≥m t·ªët h∆°n ‚Üí gi·∫£m nguy c∆° k·∫πt | Kh√¥ng ƒë·∫£m b·∫£o t·ªëi ∆∞u, k·∫øt qu·∫£ kh√¥ng ·ªïn ƒë·ªãnh gi·ªØa c√°c l·∫ßn ch·∫°y      | ƒêa s·ªë gi·∫£i ƒë∆∞·ª£c, b∆∞·ªõc dao ƒë·ªông (64‚Äì82 b∆∞·ªõc)   |
| **Beam Search**                   | Duy tr√¨ nhi·ªÅu tr·∫°ng th√°i t·ªët nh·∫•t ‚Üí gi·∫£m k·∫πt local  | C·∫ßn ch·ªçn `k` h·ª£p l√Ω, v·∫´n c√≥ th·ªÉ l·∫∑p v√† sai h∆∞·ªõng                   | Ch·∫°y ƒë∆∞·ª£c, nh∆∞ng gi·∫£i k√©m (154 b∆∞·ªõc)          |
| **Simulated Annealing**           | Cho ph√©p "nh·∫£y c√≥ ki·ªÉm so√°t" ra kh·ªèi local minima   | Ph·ª• thu·ªôc nhi·ªÅu v√†o tham s·ªë nhi·ªát ƒë·ªô, d·ªÖ sai n·∫øu c·∫•u h√¨nh ch∆∞a t·ªët | Hi·∫øm khi gi·∫£i ƒë∆∞·ª£c n·∫øu kh√¥ng t·ªëi ∆∞u tham s·ªë   |
| **Genetic Algorithm**             | Kh√°m ph√° m·∫°nh nh·ªù ƒë·ªôt bi·∫øn v√† lai gh√©p              | Ng·∫´u nhi√™n cao, kh√¥ng ƒë·∫£m b·∫£o t√¨m ra l·ªùi gi·∫£i, c·∫ßn th·ª≠ nhi·ªÅu l·∫ßn   | R·∫•t hi·∫øm khi gi·∫£i ra, k·∫øt qu·∫£ kh√¥ng ·ªïn ƒë·ªãnh   |

‚úÖ T·ªïng k·∫øt:

C√°c thu·∫≠t to√°n c·ª•c b·ªô r·∫•t ph√π h·ª£p ƒë·ªÉ ch·∫°y nhanh tr√™n kh√¥ng gian l·ªõn, nh∆∞ng kh√¥ng ƒë·∫£m b·∫£o lu√¥n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i, nh·∫•t l√† khi:

Kh√¥ng c√≥ c∆° ch·∫ø tho√°t kh·ªèi local minima

Ho·∫∑c khi heuristic kh√¥ng ƒë·ªß d·∫´n h∆∞·ªõng

Steepest Ascent HC v√† Stochastic HC l√† nh·ªØng l·ª±a ch·ªçn kh√° hi·ªáu qu·∫£, nh∆∞ng v·∫´n ph·ª• thu·ªôc v√†o may r·ªßi v√† c·∫•u h√¨nh. Steepest HC b·ªã k·∫πt n·∫øu kh√¥ng c√≥ h√†ng x√≥m n√†o t·ªët h∆°n. Stochastic HC ng·∫´u nhi√™n n√™n c√≥ th·ªÉ kh√¥ng ch·ªçn ƒë√∫ng h∆∞·ªõng t·ªët nh·∫•t.

Simulated Annealing v√† Genetic Algorithm c·∫ßn t·ªëi ∆∞u tham s·ªë, ch·∫°y nhi·ªÅu l·∫ßn, v√† v·∫´n kh√¥ng ƒë·∫£m b·∫£o th√†nh c√¥ng.

### 2.4. T√¨m ki·∫øm m√† kh√¥ng quan s√°t: Tr·∫°ng th√°i ni·ªÅm tin (Belief State)

#### 2.4.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm (trong m√¥i tr∆∞·ªùng kh√¥ng quan s√°t)

| Th√†nh ph·∫ßn                                        | M√¥ t·∫£                                                                                                                                                        |
| ------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| **Tr·∫°ng th√°i ban ƒë·∫ßu (Initial State)**            | L√† **m·ªôt t·∫≠p h·ª£p c√°c tr·∫°ng th√°i c√≥ th·ªÉ** x·∫£y ra, v√¨ agent kh√¥ng bi·∫øt ch·∫Øc m√¨nh ƒëang ·ªü ƒë√¢u. ƒê√¢y ƒë∆∞·ª£c g·ªçi l√† **tr·∫°ng th√°i ni·ªÅm tin (belief state)**.           |
| **Tr·∫°ng th√°i ƒë√≠ch (Goal State)**                  | C≈©ng l√† m·ªôt t·∫≠p h·ª£p c√°c tr·∫°ng th√°i ƒë∆∞·ª£c ch·∫•p nh·∫≠n l√† m·ª•c ti√™u. Agent ph·∫£i ch·∫Øc ch·∫Øn r·∫±ng m√¨nh ƒë√£ v√†o m·ªôt trong c√°c tr·∫°ng th√°i n√†y sau khi k·∫øt th√∫c k·∫ø ho·∫°ch. |
| **Tr·∫°ng th√°i hi·ªán t·∫°i (Current Belief State)**    | T·∫°i m·ªçi th·ªùi ƒëi·ªÉm, agent kh√¥ng bi·∫øt ƒëang ·ªü tr·∫°ng th√°i n√†o, m√† ch·ªâ bi·∫øt ƒëang n·∫±m trong **m·ªôt t·∫≠p tr·∫°ng th√°i kh·∫£ dƒ©**.                                         |
| **T·∫≠p h√†nh ƒë·ªông (Actions)**                       | T·∫≠p c√°c h√†nh ƒë·ªông √°p d·ª•ng ƒë∆∞·ª£c trong m√¥i tr∆∞·ªùng (v√≠ d·ª•: di chuy·ªÉn √¥ tr·ªëng l√™n, xu·ªëng, tr√°i, ph·∫£i).                                                           |
| **H√†m k·∫ø ti·∫øp (Transition / Successor function)** | M·ªôt h√†nh ƒë·ªông ƒë∆∞·ª£c √°p d·ª•ng l√™n t·∫•t c·∫£ c√°c tr·∫°ng th√°i trong belief state hi·ªán t·∫°i ƒë·ªÉ t·∫°o ra belief state m·ªõi.                                                 |
| **Ki·ªÉm tra m·ª•c ti√™u (Goal Test)**                 | M·ªôt belief state l√† ƒë·∫°t m·ª•c ti√™u n·∫øu **to√†n b·ªô c√°c tr·∫°ng th√°i trong n√≥ n·∫±m trong t·∫≠p goal**. T·ª©c l√†: `belief_state ‚äÜ goal_set`.                              |
| **Chi ph√≠ (Cost)**                                | M·ªói h√†nh ƒë·ªông c√≥ th·ªÉ c√≥ chi ph√≠ c·ªë ƒë·ªãnh (v√≠ d·ª•: 1). T·ªïng chi ph√≠ l√† ƒë·ªô d√†i c·ªßa k·∫ø ho·∫°ch.                                                                     |

#### 2.4.2. Gi·∫£i ph√°p (Solution) l√† g√¨?

Trong t√¨m ki·∫øm kh√¥ng quan s√°t, solution l√† m·ªôt chu·ªói h√†nh ƒë·ªông (k·∫ø ho·∫°ch) [a1, a2, ..., an] th·ªèa m√£n: Sau khi th·ª±c hi·ªán c√°c h√†nh ƒë·ªông n√†y, b·∫•t k·ªÉ tr·∫°ng th√°i ban ƒë·∫ßu l√† g√¨, agent ch·∫Øc ch·∫Øn s·∫Ω k·∫øt th√∫c ·ªü m·ªôt tr·∫°ng th√°i n·∫±m trong t·∫≠p goal.

C·ª• th·ªÉ: apply(plan, initial_beliefs) ‚äÜ goal_beliefs

T·ª©c l√†: n·∫øu b·∫Øt ƒë·∫ßu t·ª´ b·∫•t k·ª≥ tr·∫°ng th√°i n√†o trong initial_beliefs, sau khi th·ª±c hi·ªán chu·ªói h√†nh ƒë·ªông, t·∫•t c·∫£ tr·∫°ng th√°i kh·∫£ dƒ© ph·∫£i thu·ªôc goal_beliefs.

üìå Kh√°c bi·ªát so v·ªõi t√¨m ki·∫øm th√¥ng th∆∞·ªùng:

| Thu·ªôc t√≠nh          | T√¨m ki·∫øm quan s√°t ƒë∆∞·ª£c                  | T√¨m ki·∫øm kh√¥ng quan s√°t                       |
| ------------------- | --------------------------------------- | --------------------------------------------- |
| Tr·∫°ng th√°i hi·ªán t·∫°i | Bi·∫øt ch√≠nh x√°c                          | Kh√¥ng bi·∫øt ch√≠nh x√°c                          |
| H√†nh ƒë·ªông ph·∫£n h·ªìi  | C√≥ th·ªÉ ƒëi·ªÅu ch·ªânh theo k·∫øt qu·∫£ quan s√°t | Kh√¥ng th·ªÉ ƒëi·ªÅu ch·ªânh (v√¨ kh√¥ng quan s√°t ƒë∆∞·ª£c) |
| K·∫ø ho·∫°ch            | C√≥ th·ªÉ ph√¢n nh√°nh (contingent)          | Ph·∫£i c·ªë ƒë·ªãnh cho m·ªçi kh·∫£ nƒÉng (conformant)    |

üß† V√≠ d·ª• minh h·ªça ƒë∆°n gi·∫£n (8-Puzzle): C·∫ßn t√¨m m·ªôt chu·ªói h√†nh ƒë·ªông duy nh·∫•t sao cho: sau khi √°p d·ª•ng n√≥ l√™n m·ªçi kh·∫£ nƒÉng, t·∫•t c·∫£ k·∫øt qu·∫£ ƒë·ªÅu ƒë√∫ng ƒë√≠ch.

### 2.5. T√¨m ki·∫øm v·ªõi quan s√°t m·ªôt ph·∫ßn: Tr·∫°ng th√°i ni·ªÅm tin (Belief State)

#### 2.5.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm

Trong m√¥i tr∆∞·ªùng quan s√°t m·ªôt ph·∫ßn, agent ch·ªâ bi·∫øt m·ªôt ph·∫ßn tr·∫°ng th√°i ban ƒë·∫ßu, v√† s·∫Ω d·∫ßn c·∫≠p nh·∫≠t ni·ªÅm tin (belief state) khi nh·∫≠n ƒë∆∞·ª£c c√°c quan s√°t sau m·ªói h√†nh ƒë·ªông.

| Th√†nh ph·∫ßn                                                                                                                                                                                                                      | M√¥ t·∫£                                                                                                                              |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------- |
| **Tr·∫°ng th√°i th·∫≠t (Actual state)**                                                                                                                                                                                              | Tr·∫°ng th√°i th·ª±c t·∫ø m√† agent ƒëang ·ªü trong, nh∆∞ng kh√¥ng bi·∫øt ho√†n to√†n.                                                              |
| **Tr·∫°ng th√°i ni·ªÅm tin (Belief state)**                                                                                                                                                                                          | T·∫≠p h·ª£p c√°c tr·∫°ng th√°i m√† agent **tin r·∫±ng c√≥ th·ªÉ m√¨nh ƒëang ·ªü ƒë√≥**, d·ª±a tr√™n ph·∫ßn th√¥ng tin ƒë√£ bi·∫øt v√† quan s√°t sau m·ªói h√†nh ƒë·ªông. |
| **Tr·∫°ng th√°i ban ƒë·∫ßu (Initial belief)**                                                                                                                                                                                         | L√† t·∫≠p h·ª£p c√°c tr·∫°ng th√°i c√≥ chung ph·∫ßn ƒë√£ bi·∫øt.                                                                                   |
| ‚Üí V√≠ d·ª•: n·∫øu bi·∫øt 3 ph·∫ßn t·ª≠ ƒë·∫ßu ti√™n c·ªßa h√†ng ƒë·∫ßu l√† `[2, 6, 5]`, th√¨ agent s·∫Ω sinh t·∫•t c·∫£ c√°c tr·∫°ng th√°i 8-Puzzle c√≥ h√†ng ƒë·∫ßu l√† `[2, 6, 5]`, v√† ph·∫ßn c√≤n l·∫°i l√† m·ªçi ho√°n v·ªã h·ª£p l·ªá c·ªßa c√°c s·ªë c√≤n l·∫°i (`{0, 1, 3, 4, 7, 8}`). |                                                                                                                                    |
| **T·∫≠p h√†nh ƒë·ªông (Actions)**                                                                                                                                                                                                     | C√°c thao t√°c di chuy·ªÉn √¥ tr·ªëng (l√™n, xu·ªëng, tr√°i, ph·∫£i).                                                                           |
| **T·∫≠p quan s√°t (Observations)**                                                                                                                                                                                                 | Sau m·ªói h√†nh ƒë·ªông, agent nh·∫≠n ƒë∆∞·ª£c th√¥ng tin m·ªôt ph·∫ßn v·ªÅ tr·∫°ng th√°i m·ªõi (v√≠ d·ª•: nh√¨n th·∫•y c√°c √¥ xung quanh √¥ tr·ªëng).               |
| **H√†m k·∫ø ti·∫øp (Transition function)**                                                                                                                                                                                           | Cho bi·∫øt c√°c tr·∫°ng th√°i m·ªõi c√≥ th·ªÉ sau khi th·ª±c hi·ªán h√†nh ƒë·ªông.                                                                    |
| **H√†m quan s√°t (Observation function)**                                                                                                                                                                                         | D·ª±a v√†o tr·∫°ng th√°i th·∫≠t, tr·∫£ v·ªÅ m·ªôt quan s√°t c·ª• th·ªÉ m√† agent nh√¨n th·∫•y ƒë∆∞·ª£c.                                                       |
| **H√†m c·∫≠p nh·∫≠t ni·ªÅm tin (Belief update)**                                                                                                                                                                                       | D·ª±a v√†o h√†nh ƒë·ªông v·ª´a th·ª±c hi·ªán v√† quan s√°t nh·∫≠n ƒë∆∞·ª£c ‚Üí lo·∫°i b·ªè c√°c tr·∫°ng th√°i kh√¥ng ph√π h·ª£p kh·ªèi belief state.                    |
| **Tr·∫°ng th√°i m·ª•c ti√™u (Goal states)**                                                                                                                                                                                           | T·∫≠p h·ª£p c√°c tr·∫°ng th√°i ƒë∆∞·ª£c xem l√† ho√†n th√†nh m·ª•c ti√™u.                                                                            |
| **Ki·ªÉm tra m·ª•c ti√™u (Goal test)**                                                                                                                                                                                               | Khi belief state ch·ªâ c√≤n c√°c tr·∫°ng th√°i thu·ªôc goal ‚Üí xem l√† ƒë√£ ho√†n th√†nh.                                                         |

#### 2.5.2. Solution l√† g√¨?

Solution l√† m·ªôt k·∫ø ho·∫°ch c√≥ ƒëi·ªÅu ki·ªán (contingent plan), g·ªìm c√°c h√†nh ƒë·ªông v√† nh√°nh r·∫Ω theo quan s√°t, gi√∫p agent t·ª´ tr·∫°ng th√°i ni·ªÅm tin ban ƒë·∫ßu d·∫ßn thu h·∫πp t·ªõi tr·∫°ng th√°i goal.

T·ª©c l√†:

&nbsp;&nbsp;Kh√¥ng ph·∫£i m·ªôt chu·ªói c·ªë ƒë·ªãnh [a1, a2, ..., an]

&nbsp;&nbsp;M√† l√† m·ªôt c√¢y k·∫ø ho·∫°ch d·∫°ng:

&nbsp;&nbsp;&nbsp;&nbsp;Th·ª±c hi·ªán a1

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üí n·∫øu quan s√°t o1: l√†m a2

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;‚Üí n·∫øu quan s√°t o2: l√†m a3

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;...

K·∫ø ho·∫°ch ph·∫£i ƒë·∫£m b·∫£o r·∫±ng: D√π agent b·∫Øt ƒë·∫ßu ·ªü b·∫•t k·ª≥ tr·∫°ng th√°i n√†o ph√π h·ª£p v·ªõi th√¥ng tin ban ƒë·∫ßu, sau m·ªôt chu·ªói h√†nh ƒë·ªông v√† quan s√°t, agent s·∫Ω bi·∫øt ch·∫Øc m√¨nh ƒëang ·ªü tr·∫°ng th√°i goal.

### 2.6. T√¨m ki·∫øm v·ªõi c√°c h√†nh ƒë·ªông kh√¥ng x√°c ƒë·ªãnh: C√¢y t√¨m ki·∫øm And-Or

#### 2.6.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm (v·ªõi h√†nh ƒë·ªông kh√¥ng x√°c ƒë·ªãnh)

Trong m√¥i tr∆∞·ªùng n√†y, m·ªôt h√†nh ƒë·ªông kh√¥ng lu√¥n t·∫°o ra k·∫øt qu·∫£ duy nh·∫•t. Thay v√†o ƒë√≥, m·ªói h√†nh ƒë·ªông c√≥ th·ªÉ d·∫´n t·ªõi nhi·ªÅu tr·∫°ng th√°i kh√°c nhau, g·ªçi l√† c√°c k·∫øt qu·∫£ kh·∫£ dƒ© (possible outcomes).

| Th√†nh ph·∫ßn                                      | M√¥ t·∫£                                                                         |
| ----------------------------------------------- | ----------------------------------------------------------------------------- |
| **Tr·∫°ng th√°i (State)**                          | L√† c·∫•u h√¨nh c·ªßa th·∫ø gi·ªõi hi·ªán t·∫°i (v√≠ d·ª•: tr·∫°ng th√°i 8-Puzzle).               |
| **Tr·∫°ng th√°i ban ƒë·∫ßu (Initial state)**          | Tr·∫°ng th√°i xu·∫•t ph√°t c·ªßa agent.                                               |
| **T·∫≠p h√†nh ƒë·ªông (Actions)**                     | C√°c thao t√°c m√† agent c√≥ th·ªÉ th·ª±c hi·ªán.                                       |
| **H√†m chuy·ªÉn tr·∫°ng th√°i (Transition function)** | V·ªõi m·ªói `(state, action)`, c√≥ th·ªÉ tr·∫£ v·ªÅ **nhi·ªÅu tr·∫°ng th√°i k·∫øt qu·∫£ kh·∫£ dƒ©**. |
| V√≠ d·ª•: `Result(s, a) = {s1, s2}`.               |                                                                               |
| **Ki·ªÉm tra m·ª•c ti√™u (Goal test)**               | Ki·ªÉm tra xem tr·∫°ng th√°i hi·ªán t·∫°i c√≥ n·∫±m trong t·∫≠p goal kh√¥ng.                 |
| **Chi ph√≠ b∆∞·ªõc ƒëi (Cost)**                      | C√≥ th·ªÉ t√≠nh theo h√†nh ƒë·ªông ho·∫∑c theo x√°c su·∫•t.                                |

‚ùó H√†nh ƒë·ªông kh√¥ng x√°c ƒë·ªãnh l√† g√¨?

M·ªôt h√†nh ƒë·ªông c√≥ th·ªÉ d·∫´n t·ªõi nhi·ªÅu tr·∫°ng th√°i kh√°c nhau, kh√¥ng ch·∫Øc ch·∫Øn tr∆∞·ªõc khi th·ª±c hi·ªán.

V√≠ d·ª• trong robot: "di chuy·ªÉn l√™n" ‚Üí c√≥ th·ªÉ th√†nh c√¥ng ho·∫∑c tr∆∞·ª£t sang ph·∫£i.

#### 2.6.2. Solution l√† g√¨?

Solution l√† m·ªôt c√¢y k·∫ø ho·∫°ch c√≥ nh√°nh, g·ªçi l√† And-Or search tree.

T·∫°i m·ªói n√∫t OR: agent ch·ªçn m·ªôt h√†nh ƒë·ªông

T·∫°i m·ªói nh√°nh AND: m√¥ h√¨nh h√≥a t·∫•t c·∫£ k·∫øt qu·∫£ c√≥ th·ªÉ c·ªßa h√†nh ƒë·ªông ƒë√≥

K·∫ø ho·∫°ch ch·ªâ ƒë∆∞·ª£c ch·∫•p nh·∫≠n n·∫øu t·∫•t c·∫£ c√°c nh√°nh c·ªßa m·ªçi AND-node d·∫´n ƒë·∫øn goal

‚Üí t·ª©c l√† ƒë·∫£m b·∫£o th√†nh c√¥ng trong m·ªçi t√¨nh hu·ªëng b·∫•t ƒë·ªãnh

üìò V√≠ d·ª•:

Gi·∫£ s·ª≠:

Tr·∫°ng th√°i ban ƒë·∫ßu l√† s0

Th·ª±c hi·ªán h√†nh ƒë·ªông a1 c√≥ th·ªÉ d·∫´n ƒë·∫øn {s1, s2}

K·∫ø ho·∫°ch ph·∫£i x·ª≠ l√Ω ƒë∆∞·ª£c c·∫£ s1 v√† s2

C√¢y s·∫Ω tr√¥ng nh∆∞:

s0

‚îî‚îÄ‚îÄ a1 (OR)

    ‚îú‚îÄ‚îÄ s1 (AND) ‚îÄ‚îÄ> k·∫ø ho·∫°ch ti·∫øp theo cho s1

    ‚îî‚îÄ‚îÄ s2 (AND) ‚îÄ‚îÄ> k·∫ø ho·∫°ch ti·∫øp theo cho s2

‚úÖ Ch·ªâ khi c·∫£ s1 v√† s2 ƒë·ªÅu c√≥ k·∫ø ho·∫°ch ƒë·∫øn goal, th√¨ k·∫ø ho·∫°ch t·ª´ s0 m·ªõi ƒë∆∞·ª£c ch·∫•p nh·∫≠n.

### 2.7. Constraint Satisfaction Problem (CSP): AC-3

#### 2.7.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm trong CSP

M·ªôt b√†i to√°n r√†ng bu·ªôc (CSP) kh√¥ng t√¨m ki·∫øm theo h√†nh ƒë·ªông t·ª´ng b∆∞·ªõc nh∆∞ 8-Puzzle, m√† thay v√†o ƒë√≥ l√† t√¨m gi√° tr·ªã cho bi·∫øn sao cho th·ªèa m√£n r√†ng bu·ªôc. Th√†nh ph·∫ßn c·ªßa m·ªôt CSP g·ªìm:

| Th√†nh ph·∫ßn                      | M√¥ t·∫£                                                            |
| ------------------------------- | ---------------------------------------------------------------- |
| **T·∫≠p bi·∫øn (Variables)**        | C√°c bi·∫øn c·∫ßn g√°n gi√° tr·ªã. V√≠ d·ª•: `X1, X2, ..., Xn`.              |
| **Mi·ªÅn gi√° tr·ªã (Domains)**      | V·ªõi m·ªói bi·∫øn, m·ªôt t·∫≠p c√°c gi√° tr·ªã kh·∫£ dƒ©. V√≠ d·ª•: `X1 ‚àà {1,2,3}`. |
| **T·∫≠p r√†ng bu·ªôc (Constraints)** | C√°c ƒëi·ªÅu ki·ªán m√† c√°c bi·∫øn ph·∫£i th·ªèa m√£n. |

üìò V√≠ d·ª• ƒë∆°n gi·∫£n:

&nbsp;&nbsp;&nbsp;Bi·∫øn: A, B, C

&nbsp;&nbsp;&nbsp;Mi·ªÅn: {1, 2, 3}

&nbsp;&nbsp;&nbsp;R√†ng bu·ªôc:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A ‚â† B

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;B ‚â† C

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;A ‚â† C

&nbsp;&nbsp;&nbsp;üëâ B√†i to√°n y√™u c·∫ßu t√¨m gi√° tr·ªã cho A, B, C sao cho kh√¥ng c√≥ 2 bi·∫øn n√†o tr√πng nhau.

#### 2.7.2. Thu·∫≠t to√°n AC-3 l√† g√¨ ?

AC-3 (Arc Consistency 3) l√† m·ªôt thu·∫≠t to√°n l·ªçc mi·ªÅn gi√° tr·ªã ƒë·ªÉ ƒë∆°n gi·∫£n h√≥a CSP tr∆∞·ªõc khi t√¨m l·ªùi gi·∫£i, b·∫±ng c√°ch lo·∫°i b·ªè nh·ªØng gi√° tr·ªã ch·∫Øc ch·∫Øn kh√¥ng th·ªÉ ƒë∆∞·ª£c ch·ªçn.

üìå C∆° ch·∫ø ho·∫°t ƒë·ªông c·ªßa AC-3:

&nbsp;&nbsp;&nbsp;X√©t t·∫•t c·∫£ c√°c c·∫∑p bi·∫øn c√≥ r√†ng bu·ªôc (Xi, Xj)

&nbsp;&nbsp;&nbsp;V·ªõi m·ªói c·∫∑p, ki·ªÉm tra xem t·∫•t c·∫£ gi√° tr·ªã trong Xi c√≥ ‚Äúh·ª£p l·ªá‚Äù v·ªõi √≠t nh·∫•t 1 gi√° tr·ªã c·ªßa Xj kh√¥ng

&nbsp;&nbsp;&nbsp;N·∫øu c√≥ gi√° tr·ªã trong mi·ªÅn Xi m√† kh√¥ng c√≤n t∆∞∆°ng th√≠ch v·ªõi Xj n√†o ‚Üí lo·∫°i b·ªè n√≥ kh·ªèi mi·ªÅn Xi

&nbsp;&nbsp;&nbsp;Qu√° tr√¨nh n√†y l·∫∑p l·∫°i cho ƒë·∫øn khi kh√¥ng c√≤n thay ƒë·ªïi n√†o.

#### 2.7.3. Solution l√† g√¨ ?

Solution c·ªßa b√†i to√°n CSP l√† m·ªôt √°nh x·∫° t·ª´ bi·∫øn ‚Üí gi√° tr·ªã, sao cho t·∫•t c·∫£ c√°c r√†ng bu·ªôc ƒë∆∞·ª£c th·ªèa m√£n.

N·∫øu d√πng AC-3:

&nbsp;&nbsp;&nbsp;C√≥ th·ªÉ ƒë∆°n gi·∫£n b√†i to√°n tr∆∞·ªõc b·∫±ng c√°ch gi·∫£m mi·ªÅn

&nbsp;&nbsp;&nbsp;Sau ƒë√≥ k·∫øt h·ª£p v·ªõi c√°c k·ªπ thu·∫≠t kh√°c nh∆∞ Backtracking, Forward Checking ƒë·ªÉ t√¨m l·ªùi gi·∫£i ƒë·∫ßy ƒë·ªß

üß† T√≥m t·∫Øt:

| Th√†nh ph·∫ßn  | √ù nghƒ©a trong CSP                                          |
| ----------- | ---------------------------------------------------------- |
| Variables   | C√°c bi·∫øn c·∫ßn g√°n gi√° tr·ªã                                   |
| Domains     | T·∫≠p gi√° tr·ªã h·ª£p l·ªá ban ƒë·∫ßu c·ªßa m·ªói bi·∫øn                    |
| Constraints | C√°c ƒëi·ªÅu ki·ªán gi·ªØa c√°c bi·∫øn                                |
| AC-3        | B·ªô l·ªçc ki·ªÉm tra v√† c·∫Øt gi·∫£m mi·ªÅn kh√¥ng c·∫ßn thi·∫øt           |
| Solution    | G√°n gi√° tr·ªã cho m·ªçi bi·∫øn sao cho t·∫•t c·∫£ r√†ng bu·ªôc ƒë·ªÅu ƒë√∫ng |

### 2.8. Constraint Satisfaction Problem (CSP): Forward Checking

#### 2.8.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm CSP

B√†i to√°n CSP (Constraint Satisfaction Problem ‚Äì b√†i to√°n th·ªèa m√£n r√†ng bu·ªôc) l√† m·ªôt d·∫°ng b√†i to√°n trong tr√≠ tu·ªá nh√¢n t·∫°o, trong ƒë√≥ c·∫ßn t√¨m gi√° tr·ªã ph√π h·ª£p cho t·∫≠p bi·∫øn sao cho th·ªèa m√£n t·∫•t c·∫£ c√°c r√†ng bu·ªôc ƒë√£ cho.

| Th√†nh ph·∫ßn                      | M√¥ t·∫£                                                                                                  |
| ------------------------------- | ------------------------------------------------------------------------------------------------------ |
| **T·∫≠p bi·∫øn (Variables)**        | C√°c bi·∫øn c·∫ßn ƒë∆∞·ª£c g√°n gi√° tr·ªã, v√≠ d·ª•: `X1, X2, ..., Xn`.                                               |
| **Mi·ªÅn gi√° tr·ªã (Domains)**      | M·ªói bi·∫øn c√≥ m·ªôt t·∫≠p gi√° tr·ªã h·ª£p l·ªá ban ƒë·∫ßu. V√≠ d·ª•: `X1 ‚àà {1, 2, 3}`.                                   |
| **T·∫≠p r√†ng bu·ªôc (Constraints)** | C√°c ƒëi·ªÅu ki·ªán gi·ªØa c√°c bi·∫øn. V√≠ d·ª•: `X1 ‚â† X2`, `X1 + X2 = X3`, `alldiff(X1, X2, X3)`.                  |
| **M·ª•c ti√™u (Goal)**             | T√¨m ƒë∆∞·ª£c m·ªôt √°nh x·∫° (assignment) t·ª´ bi·∫øn ‚Üí gi√° tr·ªã sao cho **t·∫•t c·∫£ c√°c r√†ng bu·ªôc ƒë·ªÅu ƒë∆∞·ª£c th·ªèa m√£n**. |

#### 2.8.2. Forward Checking l√† g√¨ ?

Forward Checking (FC) l√† m·ªôt k·ªπ thu·∫≠t h·ªó tr·ª£ trong qu√° tr√¨nh t√¨m ki·∫øm l·ªùi gi·∫£i CSP, d√πng ƒë·ªÉ ph√°t hi·ªán m√¢u thu·∫´n s·ªõm b·∫±ng c√°ch c·∫≠p nh·∫≠t mi·ªÅn (domain) c·ªßa c√°c bi·∫øn ch∆∞a g√°n ngay sau m·ªói l·∫ßn g√°n.

üìò C√°ch ho·∫°t ƒë·ªông c·ªßa Forward Checking:
&nbsp;&nbsp;&nbsp;Khi g√°n m·ªôt gi√° tr·ªã cho bi·∫øn X, Forward Checking s·∫Ω:

&nbsp;&nbsp;&nbsp;D√≤ qua c√°c bi·∫øn ch∆∞a g√°n nh∆∞ng c√≥ r√†ng bu·ªôc v·ªõi X

&nbsp;&nbsp;&nbsp;Lo·∫°i b·ªè kh·ªèi domain c·ªßa c√°c bi·∫øn ƒë√≥ nh·ªØng gi√° tr·ªã kh√¥ng c√≤n h·ª£p l·ªá (d·∫´n ƒë·∫øn vi ph·∫°m r√†ng bu·ªôc)

&nbsp;&nbsp;&nbsp;N·∫øu m·ªôt bi·∫øn b·ªã r√∫t g·ªçn domain v·ªÅ r·ªóng, th√¨ bi·∫øt r·∫±ng l·ª±a ch·ªçn hi·ªán t·∫°i l√† sai ‚Üí backtrack s·ªõm

üß† V√≠ d·ª• ƒë∆°n gi·∫£n:

&nbsp;&nbsp;&nbsp;Bi·∫øn: A, B, C

&nbsp;&nbsp;&nbsp;Mi·ªÅn: {1, 2, 3}

&nbsp;&nbsp;&nbsp;R√†ng bu·ªôc: A ‚â† B, B ‚â† C, A ‚â† C

&nbsp;&nbsp;&nbsp;Qu√° tr√¨nh:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G√°n A = 1

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;FC s·∫Ω:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Lo·∫°i 1 kh·ªèi domain c·ªßa B v√† C

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N·∫øu c√≤n domain h·ª£p l·ªá ‚Üí ti·∫øp t·ª•c

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;N·∫øu c√≥ domain n√†o r·ªóng ‚Üí backtrack

#### 2.8.3. Solution l√† g√¨ ?

Solution c·ªßa b√†i to√°n CSP khi d√πng Forward Checking l√† m·ªôt g√°n gi√° tr·ªã ho√†n ch·ªânh cho t·∫•t c·∫£ c√°c bi·∫øn sao cho m·ªçi r√†ng bu·ªôc ƒë·ªÅu ƒë∆∞·ª£c th·ªèa m√£n.

Forward Checking kh√¥ng thay ƒë·ªïi ƒë·ªãnh nghƒ©a solution, m√† ch·ªâ tƒÉng hi·ªáu qu·∫£ t√¨m ki·∫øm b·∫±ng c√°ch:

&nbsp;&nbsp;&nbsp;D√≤ m√¢u thu·∫´n s·ªõm

&nbsp;&nbsp;&nbsp;C·∫Øt nh√°nh t√¨m ki·∫øm kh√¥ng c·∫ßn thi·∫øt

&nbsp;&nbsp;&nbsp;K·∫øt h·ª£p t·ªët v·ªõi Backtracking

### 2.9. Constraint Satisfaction Problem (CSP): Backtracking

#### 2.9.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n t√¨m ki·∫øm CSP

Trong CSP (Constraint Satisfaction Problem), b√†i to√°n ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a b·∫±ng ba th√†nh ph·∫ßn c∆° b·∫£n:

| Th√†nh ph·∫ßn                      | M√¥ t·∫£                                                                                                    |
| ------------------------------- | -------------------------------------------------------------------------------------------------------- |
| **T·∫≠p bi·∫øn (Variables)**        | C√°c bi·∫øn c·∫ßn ƒë∆∞·ª£c g√°n gi√° tr·ªã. V√≠ d·ª•: `X1, X2, ..., Xn`.                                                 |
| **Mi·ªÅn gi√° tr·ªã (Domains)**      | T·∫≠p gi√° tr·ªã kh·∫£ dƒ© cho m·ªói bi·∫øn. V√≠ d·ª•: `X1 ‚àà {1, 2, 3}`.                                                |
| **T·∫≠p r√†ng bu·ªôc (Constraints)** | C√°c ƒëi·ªÅu ki·ªán c·∫ßn ƒë∆∞·ª£c th·ªèa m√£n gi·ªØa c√°c bi·∫øn, v√≠ d·ª•: `X1 ‚â† X2`, `X3 + X4 = 5`, `alldiff(X1, X2, X3)`... |
| **M·ª•c ti√™u (Goal)**             | T√¨m m·ªôt g√°n gi√° tr·ªã cho t·∫•t c·∫£ c√°c bi·∫øn sao cho **t·∫•t c·∫£ c√°c r√†ng bu·ªôc ƒë·ªÅu ƒë∆∞·ª£c th·ªèa m√£n**.              |

#### 2.9.2. Backtracking l√† g√¨ ?

Backtracking l√† m·ªôt k·ªπ thu·∫≠t gi·∫£i b√†i to√°n CSP b·∫±ng c√°ch g√°n gi√° tr·ªã tu·∫ßn t·ª± cho t·ª´ng bi·∫øn, v√† l√πi l·∫°i (backtrack) khi ph√°t hi·ªán vi ph·∫°m r√†ng bu·ªôc.

üìò C√°ch ho·∫°t ƒë·ªông:
&nbsp;&nbsp;&nbsp;B·∫Øt ƒë·∫ßu v·ªõi bi·∫øn ƒë·∫ßu ti√™n ch∆∞a g√°n

&nbsp;&nbsp;&nbsp;Th·ª≠ t·ª´ng gi√° tr·ªã trong domain

&nbsp;&nbsp;&nbsp;Ki·ªÉm tra r√†ng bu·ªôc v·ªõi c√°c bi·∫øn ƒë√£ g√°n

&nbsp;&nbsp;&nbsp;N·∫øu h·ª£p l·ªá ‚Üí ti·∫øp t·ª•c v·ªõi bi·∫øn k·∫ø ti·∫øp

&nbsp;&nbsp;&nbsp;N·∫øu kh√¥ng c√≥ gi√° tr·ªã h·ª£p l·ªá ‚Üí quay lui (backtrack) v·ªÅ bi·∫øn tr∆∞·ªõc ƒë·ªÉ th·ª≠ gi√° tr·ªã kh√°c

üß† V√≠ d·ª• ƒë∆°n gi·∫£n:
&nbsp;&nbsp;&nbsp;Bi·∫øn: A, B, C

&nbsp;&nbsp;&nbsp;Mi·ªÅn: {1, 2, 3}

&nbsp;&nbsp;&nbsp;R√†ng bu·ªôc: A ‚â† B, B ‚â† C, A ‚â† C

&nbsp;&nbsp;&nbsp;Quy tr√¨nh:

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G√°n A = 1

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G√°n B = 2 ‚Üí h·ª£p l·ªá

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;G√°n C = 1 ‚Üí vi ph·∫°m A ‚â† C

&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;Backtrack: th·ª≠ C = 3 ‚Üí h·ª£p l·ªá ‚Üí k·∫øt th√∫c

#### 2.9.3. Solution l√† g√¨ ?

Solution l√† m·ªôt √°nh x·∫° ƒë·∫ßy ƒë·ªß t·ª´ bi·∫øn ‚Üí gi√° tr·ªã sao cho t·∫•t c·∫£ r√†ng bu·ªôc ƒë∆∞·ª£c th·ªèa m√£n.

üîç ƒê·∫∑c ƒëi·ªÉm c·ªßa Backtracking:

| ∆Øu ƒëi·ªÉm                                    | Nh∆∞·ª£c ƒëi·ªÉm                          |
| ------------------------------------------ | ----------------------------------- |
| ƒê∆°n gi·∫£n, d·ªÖ c√†i ƒë·∫∑t                       | C√≥ th·ªÉ duy·ªát qua nhi·ªÅu nh√°nh v√¥ √≠ch |
| Lu√¥n t√¨m ƒë∆∞·ª£c l·ªùi gi·∫£i n·∫øu c√≥              | T·ªëc ƒë·ªô ch·∫≠m n·∫øu kh√¥ng c√≥ c·∫£i ti·∫øn   |
| K·∫øt h·ª£p t·ªët v·ªõi k·ªπ thu·∫≠t kh√°c nh∆∞ FC, AC-3 | Kh√¥ng t·ªëi ∆∞u v·ªÅ th·ªùi gian           |

### 2.10. Reinforcement Learning (H·ªçc tƒÉng c∆∞·ªùng): Q-learning

#### 2.10.1. C√°c th√†nh ph·∫ßn ch√≠nh c·ªßa b√†i to√°n h·ªçc tƒÉng c∆∞·ªùng (Reinforcement Learning)

Trong h·ªçc tƒÉng c∆∞·ªùng, m·ªôt agent (t√°c t·ª≠) h·ªçc c√°ch h√†nh ƒë·ªông t·ªëi ∆∞u th√¥ng qua t∆∞∆°ng t√°c v·ªõi m√¥i tr∆∞·ªùng, th√¥ng qua ph·∫ßn th∆∞·ªüng (reward).

B√†i to√°n ƒë∆∞·ª£c m√¥ h√¨nh h√≥a th√†nh m·ªôt MDP (Markov Decision Process) v·ªõi c√°c th√†nh ph·∫ßn ch√≠nh:

| Th√†nh ph·∫ßn                                          | M√¥ t·∫£                                                                                         |
| --------------------------------------------------- | --------------------------------------------------------------------------------------------- |
| **T·∫≠p tr·∫°ng th√°i (States ‚Äì S)**                     | T·∫≠p h·ª£p t·∫•t c·∫£ c√°c tr·∫°ng th√°i m√† agent c√≥ th·ªÉ g·∫∑p.                                            |
| **T·∫≠p h√†nh ƒë·ªông (Actions ‚Äì A)**                     | T·∫≠p c√°c h√†nh ƒë·ªông m√† agent c√≥ th·ªÉ th·ª±c hi·ªán ·ªü m·ªói tr·∫°ng th√°i.                                 |
| **H√†m chuy·ªÉn tr·∫°ng th√°i (Transition function ‚Äì T)** | X√°c su·∫•t chuy·ªÉn t·ª´ tr·∫°ng th√°i n√†y sang tr·∫°ng th√°i kh√°c sau khi th·ª±c hi·ªán m·ªôt h√†nh ƒë·ªông:       |
| \`P(s'                                              | s, a)\`                                                                                       |
| **H√†m ph·∫ßn th∆∞·ªüng (Reward function ‚Äì R)**           | Cho bi·∫øt ph·∫ßn th∆∞·ªüng agent nh·∫≠n ƒë∆∞·ª£c khi th·ª±c hi·ªán h√†nh ƒë·ªông `a` t·∫°i tr·∫°ng th√°i `s`:          |
| `R(s, a)` ho·∫∑c `R(s, a, s')`                        |                                                                                               |
| **Ch√≠nh s√°ch (Policy ‚Äì œÄ)**                         | M·ªôt chi·∫øn l∆∞·ª£c x√°c ƒë·ªãnh h√†nh ƒë·ªông c·∫ßn th·ª±c hi·ªán t·∫°i m·ªói tr·∫°ng th√°i.                           |
| **M·ª•c ti√™u (Goal)**                                 | T√¨m ch√≠nh s√°ch t·ªëi ∆∞u œÄ\* ƒë·ªÉ **t·ªëi ƒëa h√≥a t·ªïng ph·∫ßn th∆∞·ªüng k·ª≥ v·ªçng t√≠ch l≈©y theo th·ªùi gian**. |

üìò Q-learning l√† g√¨?: Q-learning l√† m·ªôt thu·∫≠t to√°n h·ªçc tƒÉng c∆∞·ªùng kh√¥ng c·∫ßn bi·∫øt tr∆∞·ªõc m√¥ h√¨nh m√¥i tr∆∞·ªùng (model-free), d√πng ƒë·ªÉ ∆∞·ªõc l∆∞·ª£ng gi√° tr·ªã t·ªëi ∆∞u c·ªßa c·∫∑p tr·∫°ng th√°i-h√†nh ƒë·ªông: Q(s, a)

üìå C√¥ng th·ª©c c·∫≠p nh·∫≠t Q-learning: 

Sau khi agent th·ª±c hi·ªán h√†nh ƒë·ªông a t·∫°i tr·∫°ng th√°i s, nh·∫≠n ph·∫ßn th∆∞·ªüng r v√† chuy·ªÉn ƒë·∫øn tr·∫°ng th√°i m·ªõi s', ta c·∫≠p nh·∫≠t:

&nbsp;&nbsp;&nbsp;Q(s, a) ‚Üê Q(s, a) + Œ± [r + Œ≥ * max_a' Q(s', a') - Q(s, a)]

Trong ƒë√≥:

&nbsp;&nbsp;&nbsp;Œ± l√† learning rate (t·ªëc ƒë·ªô h·ªçc)

&nbsp;&nbsp;&nbsp;Œ≥ l√† discount factor (m·ª©c ƒë·ªô ∆∞u ti√™n t∆∞∆°ng lai)

&nbsp;&nbsp;&nbsp;max_a' Q(s', a') l√† gi√° tr·ªã h√†nh ƒë·ªông t·ªët nh·∫•t ·ªü tr·∫°ng th√°i k·∫ø ti·∫øp

#### 2.10.2. Solution l√† g√¨ ?

Solution trong Q-learning l√† b·∫£ng Q(s, a) ‚Äì ch·ª©a gi√° tr·ªã k·ª≥ v·ªçng c·ªßa vi·ªác th·ª±c hi·ªán h√†nh ƒë·ªông a t·∫°i tr·∫°ng th√°i s.

T·ª´ b·∫£ng Q, ta x√¢y d·ª±ng ch√≠nh s√°ch t·ªëi ∆∞u: œÄ*(s) = argmax_a Q(s, a)

T·ª©c l√†: t·∫°i m·ªói tr·∫°ng th√°i, ch·ªçn h√†nh ƒë·ªông c√≥ gi√° tr·ªã Q l·ªõn nh·∫•t.

üîÅ Q-learning ho·∫°t ƒë·ªông nh∆∞ th·∫ø n√†o?

&nbsp;&nbsp;&nbsp;B·∫Øt ƒë·∫ßu ·ªü m·ªôt tr·∫°ng th√°i s

&nbsp;&nbsp;&nbsp;Ch·ªçn h√†nh ƒë·ªông a (theo ch√≠nh s√°ch Œµ-greedy)

&nbsp;&nbsp;&nbsp;Th·ª±c hi·ªán a, nh·∫≠n r, chuy·ªÉn ƒë·∫øn s'

&nbsp;&nbsp;&nbsp;C·∫≠p nh·∫≠t Q(s, a) b·∫±ng c√¥ng th·ª©c h·ªçc

&nbsp;&nbsp;&nbsp;L·∫∑p l·∫°i cho nhi·ªÅu b∆∞·ªõc / t·∫≠p hu·∫•n luy·ªán

## 3. K·∫øt lu·∫≠n

üéØ M·ªôt s·ªë k·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c khi th·ª±c hi·ªán project n√†y:

### 3.1. Hi·ªÉu r√µ v√† √°p d·ª•ng nhi·ªÅu nh√≥m thu·∫≠t to√°n t√¨m ki·∫øm kh√°c nhau

ƒê√£ tri·ªÉn khai v√† so s√°nh c√°c thu·∫≠t to√°n t√¨m ki·∫øm trong m√¥i tr∆∞·ªùng quan s√°t ƒë·∫ßy ƒë·ªß, g·ªìm:

T√¨m ki·∫øm kh√¥ng th√¥ng tin: BFS, DFS, IDS, UCS

T√¨m ki·∫øm c√≥ th√¥ng tin: A*, Greedy Best-First Search, IDA*

T√¨m ki·∫øm c·ª•c b·ªô: Hill Climbing (Simple, Steepest, Stochastic), Simulated Annealing, Genetic Algorithm

### 3.2. M√¥ ph·ªèng ƒë∆∞·ª£c b√†i to√°n trong m√¥i tr∆∞·ªùng kh√¥ng ch·∫Øc ch·∫Øn

C√†i ƒë·∫∑t ƒë∆∞·ª£c phi√™n b·∫£n 8-Puzzle trong m√¥i tr∆∞·ªùng kh√¥ng c√≥ quan s√°t b·∫±ng c√°ch s·ª≠ d·ª•ng tr·∫°ng th√°i ni·ªÅm tin (belief states):

Cho ph√©p nh·∫≠p nhi·ªÅu tr·∫°ng th√°i ban ƒë·∫ßu v√† tr·∫°ng th√°i ƒë√≠ch

√Åp d·ª•ng thu·∫≠t to√°n BFS tr√™n t·∫≠p belief

Tr·∫£ v·ªÅ k·∫ø ho·∫°ch ƒë·∫£m b·∫£o m·ªçi tr·∫°ng th√°i ban ƒë·∫ßu ƒë·ªÅu ƒë·∫øn goal

### 3.3. Hi·ªán th·ª±c giao di·ªán t∆∞∆°ng t√°c b·∫±ng Python

X√¢y d·ª±ng giao di·ªán b·∫±ng Tkinter cho ph√©p:

Nh·∫≠p tr·ª±c ti·∫øp tr·∫°ng th√°i ban ƒë·∫ßu v√† ƒë√≠ch

Ch·ªçn thu·∫≠t to√°n ƒë·ªÉ gi·∫£i

Hi·ªÉn th·ªã t·ª´ng b∆∞·ªõc di chuy·ªÉn v√† k·∫øt qu·∫£ tr·ª±c quan

H·ªó tr·ª£ nh·∫≠p nhi·ªÅu belief states khi ch·ªçn t√¨m ki·∫øm kh√¥ng quan s√°t

### 3.4. Kh·∫£ nƒÉng ph√¢n t√≠ch, ƒë√°nh gi√° v√† so s√°nh thu·∫≠t to√°n

ƒê√£ th·ª±c hi·ªán nhi·ªÅu l·∫ßn ch·∫°y ƒë·ªÉ:

So s√°nh t·ªëc ƒë·ªô, s·ªë b∆∞·ªõc, ƒë·ªô ·ªïn ƒë·ªãnh

Ghi nh·∫≠n tr∆∞·ªùng h·ª£p c√°c thu·∫≠t to√°n th·∫•t b·∫°i (v√≠ d·ª•: SA v√† GA hi·∫øm khi t√¨m ra l·ªùi gi·∫£i)

R√∫t ra ∆∞u v√† nh∆∞·ª£c ƒëi·ªÉm c·ªßa t·ª´ng thu·∫≠t to√°n qua th·ª±c nghi·ªám th·ª±c t·∫ø

